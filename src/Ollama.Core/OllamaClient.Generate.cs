namespace Ollama.Core;

public sealed partial class OllamaClient
{
    /// <summary>
    /// Get streaming results for the given prompt.
    /// </summary>
    /// <param name="model">The model name.</param>
    /// <param name="prompt">The prompt.</param>
    /// <param name="cancellationToken">A cancellation token that can be used to cancel the initial request or ongoing streaming operation.</param>
    /// <returns>Streaming list of completion result generated by the model</returns>
    public async Task<StreamingResponse<GenerateCompletionResponse>> GenerateCompletionStreamingAsync(string model, string prompt, CancellationToken cancellationToken = default)
    {
        return await this.GenerateCompletionStreamingAsync(new GenerateStreamingCompletionRequest
        {
            Model = model,
            Prompt = prompt
        }, cancellationToken).ConfigureAwait(false);
    }

    /// <summary>
    /// Get streaming results for the prompt using the specified request.
    /// </summary>
    /// <param name="request">The data for this completions request.</param>
    /// <param name="cancellationToken">A cancellation token that can be used to cancel the initial request or ongoing streaming operation.</param>
    /// <returns>Streaming list of completion result generated by the model</returns>
    public async Task<StreamingResponse<GenerateCompletionResponse>> GenerateCompletionStreamingAsync(GenerateStreamingCompletionRequest request, CancellationToken cancellationToken = default)
    {
        this._logger.LogDebug("Generate streaming completion");

        Argument.AssertNotNull(request, nameof(request));
        Argument.AssertNotNullOrWhiteSpace(request.Model, nameof(request.Model));
        Argument.AssertNotNullOrWhiteSpace(request.Prompt, nameof(request.Prompt));

        try
        {
            HttpRequestMessage requestMessage = request.ToHttpRequestMessage();

            (HttpResponseMessage HttpResponseMessage, string ResponseContent) response = await this.ExecuteHttpRequestAsync(requestMessage, cancellationToken).ConfigureAwait(false);

            return StreamingResponse<GenerateCompletionResponse>.CreateFromResponse(response.HttpResponseMessage, (responseMessageForEnumeration) => ServerSendEventAsyncEnumerator<GenerateCompletionResponse>.EnumerateFromSseStream(responseMessageForEnumeration, cancellationToken));
        }
        catch (HttpOperationException ex)
        {
            this._logger.LogError(ex, "Request for generate completion faild {Message}", ex.Message);

            throw;
        }
    }

    /// <summary>
    /// Get completions use specified model for the given prompt.
    /// </summary>
    /// <param name="model">The model name.</param>
    /// <param name="prompt">The prompt.</param>
    /// <param name="cancellationToken">A cancellation token that can be used to cancel the initial request or ongoing streaming operation.</param>
    /// <returns>Completion result generated by the model</returns>
    public async Task<GenerateCompletionResponse> GenerateCompletionAsync(string model, string prompt, CancellationToken cancellationToken = default)
    {
        return await this.GenerateCompletionAsync(new GenerateCompletionRequest
        {
            Model = model,
            Prompt = prompt
        }, cancellationToken).ConfigureAwait(false);
    }


    /// <summary>
    /// Get completions as configured for the given prompt.
    /// </summary>
    /// <param name="request">The data for this completions request.</param>
    /// <param name="cancellationToken">A cancellation token that can be used to cancel the initial request or ongoing streaming operation.</param>
    /// <returns>Completion result generated by the model</returns>
    /// <exception cref="DeserializationException">When deserialize the response is null.</exception>
    public async Task<GenerateCompletionResponse> GenerateCompletionAsync(GenerateCompletionRequest request, CancellationToken cancellationToken = default)
    {
        Argument.AssertNotNull(request, nameof(request));
        Argument.AssertNotNullOrWhiteSpace(request.Model, nameof(request.Model));
        Argument.AssertNotNullOrWhiteSpace(request.Prompt, nameof(request.Prompt));

        this._logger.LogDebug("Generate completion");

        try
        {
            HttpRequestMessage requestMessage = request.ToHttpRequestMessage();

            (HttpResponseMessage httpResponseMessage, string responseContent) = await this.ExecuteHttpRequestAsync(requestMessage, cancellationToken).ConfigureAwait(false);

            this._logger.LogTrace("Generate completion response content: {responseContent}", responseContent);

            GenerateCompletionResponse? generateCompletionResponse = responseContent.FromJson<GenerateCompletionResponse>();

            return generateCompletionResponse is null
                ? throw new DeserializationException(responseContent, message: $"The generate completion response content: '{responseContent}' cannot be deserialize to an instance of {nameof(GenerateCompletionResponse)}.", innerException: null)
                : generateCompletionResponse;
        }
        catch (HttpOperationException ex)
        {
            this._logger.LogError(ex, "Request for generate completion faild {Message}", ex.Message);

            throw;
        }
    }

    /// <summary>
    /// Load the speficied model into memory.
    /// </summary>
    /// <param name="model">The model name.</param>
    /// <param name="cancellationToken">A cancellation token that can be used to cancel the initial request or ongoing streaming operation.</param>
    /// <returns>The model loaded response.</returns>
    /// <exception cref="DeserializationException">When deserialize the response is null.</exception>
    public async Task<LoadModelResponse> LoadModelAsync(string model, CancellationToken cancellationToken = default)
    {
        Argument.AssertNotNull(model, nameof(model));

        this._logger.LogDebug("Load model: {model}", model);

        try
        {
            LoadModelRequest request = new()
            {
                Model = model
            };

            HttpRequestMessage requestMessage = request.ToHttpRequestMessage();

            (HttpResponseMessage httpResponseMessage, string responseContent) = await this.ExecuteHttpRequestAsync(requestMessage, cancellationToken).ConfigureAwait(false);

            this._logger.LogTrace("Load model response content: {responseContent}", responseContent);

            LoadModelResponse? loadModelResponse = responseContent.FromJson<LoadModelResponse>();

            return loadModelResponse is null
                ? throw new DeserializationException(responseContent, message: $"The loadl model response content: '{responseContent}' cannot be deserialize to an instance of {nameof(LoadModelResponse)}.", innerException: null)
                : loadModelResponse;
        }
        catch (HttpOperationException ex)
        {
            this._logger.LogError(ex, "Request for load model faild {Message}", ex.Message);

            throw;
        }
    }
}